{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f97ef9",
   "metadata": {},
   "source": [
    "## List of terminology commonly used in Natural Language Processing (NLP):\n",
    "\n",
    "1. **Tokenization**: The process of breaking text into smaller units, such as words or sentences.\n",
    "  \n",
    "2. **Stemming**: Reducing words to their root form.\n",
    "  \n",
    "3. **Lemmatization**: Similar to stemming, but reduces words to their base or dictionary form.\n",
    "  \n",
    "4. **Stopwords**: Common words (e.g., \"the\", \"and\", \"is\") often removed during text processing to focus on meaningful words.\n",
    "  \n",
    "5. **Bag of Words (BoW)**: A representation of text data where the frequency of words is used as features.\n",
    "  \n",
    "6. **TF-IDF (Term Frequency-Inverse Document Frequency)**: A statistical measure used to evaluate the importance of a word in a document relative to a collection of documents.\n",
    "  \n",
    "7. **N-grams**: Contiguous sequences of n items (words, characters, etc.) in a text.\n",
    "  \n",
    "8. **Part-of-Speech (POS) Tagging**: Assigning grammatical categories (noun, verb, adjective, etc.) to words in a sentence.\n",
    "  \n",
    "9. **Named Entity Recognition (NER)**: Identifying named entities (e.g., person names, locations, organizations) in text.\n",
    "  \n",
    "10. **Dependency Parsing**: Analyzing the grammatical structure of a sentence to determine the relationships between words.\n",
    "  \n",
    "11. **Word Embeddings**: Dense vector representations of words in a continuous vector space.\n",
    "  \n",
    "12. **Word2Vec**: A popular technique for word embeddings based on shallow neural networks.\n",
    "  \n",
    "13. **GloVe (Global Vectors for Word Representation)**: A method for word embedding based on global word co-occurrence statistics.\n",
    "  \n",
    "14. **Sequence-to-Sequence (Seq2Seq)**: A model architecture used for tasks like machine translation and summarization, consisting of an encoder and a decoder.\n",
    "  \n",
    "15. **Recurrent Neural Networks (RNNs)**: Neural networks designed to process sequential data by maintaining internal state.\n",
    "  \n",
    "16. **Long Short-Term Memory (LSTM)**: A type of RNN designed to address the vanishing gradient problem by allowing the network to retain information over long sequences.\n",
    "  \n",
    "17. **Bidirectional LSTM (BiLSTM)**: An extension of LSTM that processes input sequences in both forward and backward directions.\n",
    "  \n",
    "18. **Attention Mechanism**: A mechanism used in neural networks to focus on specific parts of the input sequence when making predictions.\n",
    "  \n",
    "19. **Transformer**: A neural network architecture based entirely on self-attention mechanisms, commonly used in tasks like machine translation and text generation.\n",
    "  \n",
    "20. **BERT (Bidirectional Encoder Representations from Transformers)**: A pre-trained Transformer-based model designed for various NLP tasks, developed by Google.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9c685",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking down a text into smaller units, which are usually words or sentences. It's a fundamental step in natural language processing (NLP) tasks. Here's an example of how tokenization can be implemented using Python's Natural Language Toolkit (NLTK) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd290da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'like', 'words', 'or', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "# Importing NLTK and downloading necessary resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Importing the word_tokenize function from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text for tokenization\n",
    "text = \"Tokenization is a crucial step in NLP. It breaks down text into smaller units like words or sentences.\"\n",
    "\n",
    "# Tokenizing the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Printing the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c3fed",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "\n",
    "- We import NLTK and download the necessary resources (in this case, the `punkt` tokenizer).\n",
    "- We import the `word_tokenize` function from NLTK, which tokenizes a text into words.\n",
    "- We define a sample text.\n",
    "- We tokenize the sample text using the `word_tokenize` function, which returns a list of tokens.\n",
    "- Finally, we print the tokens.\n",
    "\n",
    "This demonstrates a basic example of tokenization using NLTK in Python. There are also other tokenization methods and libraries available in Python, such as spaCy, which provide different tokenization strategies and functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1a1a9",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is the process of reducing words to their root or base form, which may not always be a valid word. It's commonly used in natural language processing (NLP) to normalize words. Here's an example of how stemming can be implemented using Python's NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56903162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easili', 'consist', 'univers', 'definit']\n"
     ]
    }
   ],
   "source": [
    "# Importing NLTK and downloading necessary resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('snowball_data')\n",
    "\n",
    "# Importing the SnowballStemmer from NLTK\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Creating a SnowballStemmer object for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Sample words for stemming\n",
    "words = [\"running\", \"easily\", \"consistently\", \"universally\", \"definitions\"]\n",
    "\n",
    "# Stemming the words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Printing the stemmed words\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfe4858",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "\n",
    "- We import NLTK and download the necessary resources, including the `snowball_data` for the Snowball stemmer.\n",
    "- We import the `SnowballStemmer` class from NLTK, which provides stemming functionality.\n",
    "- We create a `SnowballStemmer` object for the English language.\n",
    "- We define a list of sample words.\n",
    "- We stem each word in the list using the `stem` method of the `SnowballStemmer` object, which reduces the words to their base forms.\n",
    "- Finally, we print the stemmed words.\n",
    "\n",
    "This demonstrates a basic example of stemming using NLTK's Snowball stemmer in Python. Keep in mind that stemming may not always produce valid words, as it focuses on word normalization rather than maintaining linguistic accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07848d",
   "metadata": {},
   "source": [
    "## other stemming techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afb790",
   "metadata": {},
   "source": [
    "1. **PorterStemmer:**\n",
    "   The Porter stemming algorithm, developed by Martin Porter, is one of the most widely used stemming algorithms. It's a rule-based algorithm that applies a series of suffix-stripping rules to reduce words to their stems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff56377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easili', 'consist', 'univers', 'definit']\n"
     ]
    }
   ],
   "source": [
    "# Importing the PorterStemmer from NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Creating a PorterStemmer object\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming the words using PorterStemmer\n",
    "stemmed_words_porter = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Printing the stemmed words using PorterStemmer\n",
    "print(stemmed_words_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f339532",
   "metadata": {},
   "source": [
    "\n",
    "2. **LancasterStemmer:**\n",
    "   The Lancaster stemming algorithm, developed by Chris D. Paice, is another widely used stemming algorithm. It's more aggressive than the Porter stemming algorithm and can sometimes produce very short stems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee56aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'easy', 'consist', 'univers', 'definit']\n"
     ]
    }
   ],
   "source": [
    "# Importing the LancasterStemmer from NLTK\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# Creating a LancasterStemmer object\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "# Stemming the words using LancasterStemmer\n",
    "stemmed_words_lancaster = [lancaster_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Printing the stemmed words using LancasterStemmer\n",
    "print(stemmed_words_lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d6d44",
   "metadata": {},
   "source": [
    "3. **RegexpStemmer:**\n",
    "   The RegexpStemmer allows stemming based on regular expressions. This can be useful when specific patterns need to be matched and stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76bb7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'easily', 'consistently', 'universally', 'definition']\n"
     ]
    }
   ],
   "source": [
    "# Importing the RegexpStemmer from NLTK\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Creating a RegexpStemmer object with a regular expression pattern\n",
    "regexp_stemmer = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "\n",
    "# Stemming the words using RegexpStemmer\n",
    "stemmed_words_regexp = [regexp_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Printing the stemmed words using RegexpStemmer\n",
    "print(stemmed_words_regexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf8efc",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16678d11",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing words to their base or dictionary form (i.e., lemma), which is linguistically correct and meaningful. Unlike stemming, lemmatization considers the context and part of speech of a word to determine its lemma. NLTK provides lemmatization functionality as well. Here's how you can perform lemmatization using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c34d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'be', 'run', 'and', 'play', 'in', 'the', 'garden']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing NLTK and downloading necessary resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Importing the WordNetLemmatizer from NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Creating a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample text for lemmatization\n",
    "text = \"The dogs are running and playing in the garden\"\n",
    "\n",
    "# Tokenizing the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Lemmatizing the words based on part-of-speech tags\n",
    "lemmatized_words = []\n",
    "for word, pos in pos_tags:\n",
    "    pos_tag_simple = 'n' if pos.startswith('N') else 'v'  # Simplifying POS tags for lemmatization\n",
    "    lemma = lemmatizer.lemmatize(word, pos_tag_simple)\n",
    "    lemmatized_words.append(lemma)\n",
    "\n",
    "# Printing the lemmatized words\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf6f43",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "\n",
    "- We import NLTK and download the necessary resources, including `punkt` for tokenization, `wordnet` for lemmatization, and `averaged_perceptron_tagger` for part-of-speech tagging.\n",
    "- We import the `WordNetLemmatizer` class from NLTK, which provides lemmatization functionality based on WordNet.\n",
    "- We define a sample text.\n",
    "- We tokenize the text into words using `word_tokenize`.\n",
    "- We perform part-of-speech tagging using `pos_tag`.\n",
    "- We iterate through the tokenized words along with their part-of-speech tags, simplify the tags, and lemmatize each word using the `lemmatize` method of the `WordNetLemmatizer` object.\n",
    "- Finally, we print the lemmatized words.\n",
    "\n",
    "This demonstrates how to perform lemmatization using NLTK in Python, considering the part of speech of each word for more accurate lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b210f88",
   "metadata": {},
   "source": [
    "# The Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306a258",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a simple and commonly used representation in natural language processing (NLP). It represents text data as a numerical vector where each dimension corresponds to a unique word in the vocabulary, and the value of each dimension represents the frequency of that word in the document. Here's how you can create a Bag of Words representation of text data using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51936b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) representation:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "\n",
      "Feature names (vocabulary):\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus (collection of documents)\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a Bag of Words representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the Bag of Words representation\n",
    "print(\"Bag of Words (BoW) representation:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(\"\\nFeature names (vocabulary):\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eaf34c",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "\n",
    "- We import `CountVectorizer` from scikit-learn, which is a tool for converting a collection of text documents into a matrix of token counts.\n",
    "- We define a sample corpus consisting of four documents.\n",
    "- We create an instance of `CountVectorizer`.\n",
    "- We fit the vectorizer to the corpus and transform the corpus into a Bag of Words representation using the `fit_transform` method.\n",
    "- We retrieve the feature names (vocabulary) using the `get_feature_names_out` method.\n",
    "- Finally, we print the Bag of Words representation and the feature names.\n",
    "\n",
    "This demonstrates how to create a Bag of Words representation of text data using Python with the help of scikit-learn's `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24397f8",
   "metadata": {},
   "source": [
    "# Other BOW Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83979e",
   "metadata": {},
   "source": [
    "There are several techniques and variations of the Bag of Words (BoW) model, each with its own characteristics and applications. Some common BoW techniques include:\n",
    "\n",
    "1. **Standard Bag of Words (BoW):** This is the basic BoW representation, where each document is represented as a vector of word counts, disregarding the order of words.\n",
    "\n",
    "2. **Binary Bag of Words:** Instead of counting the occurrences of words, this technique represents each document as a binary vector, where each element indicates whether a word is present (1) or absent (0) in the document.\n",
    "\n",
    "3. **Term Frequency-Inverse Document Frequency (TF-IDF):** TF-IDF is a variation of the BoW model that takes into account not only the frequency of words in a document but also their importance in the entire corpus. It assigns higher weights to words that are frequent in a document but rare in the corpus.\n",
    "\n",
    "4. **N-gram BoW:** This technique extends the basic BoW model by considering sequences of N consecutive words (N-grams) instead of single words. It captures some local word order information.\n",
    "\n",
    "5. **Character-level BoW:** Instead of considering words as the basic units, this technique represents documents as vectors of character counts. It can be useful when dealing with languages with complex morphology or for capturing subword information.\n",
    "\n",
    "6. **Sublinear TF Scaling:** This technique scales down the raw term frequency (TF) values to mitigate the impact of very frequent terms in the document.\n",
    "\n",
    "7. **Word Embeddings as Features:** Instead of using simple word counts, this technique represents words as dense vectors (word embeddings) learned from a large corpus using techniques like Word2Vec, GloVe, or fastText. These word embeddings capture semantic relationships between words and are often used as features in place of traditional BoW representations.\n",
    "\n",
    "These are some of the commonly used variations of the Bag of Words (BoW) model in natural language processing. Each technique has its advantages and disadvantages, and the choice of technique depends on the specific task and the characteristics of the text data being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e3d64",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27f68e",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in natural language processing and information retrieval to evaluate the importance of a word in a document relative to a collection of documents. It combines two metrics: term frequency (TF), which measures how often a term occurs in a document, and inverse document frequency (IDF), which measures the rarity of a term across the entire document collection. Here's how you can compute TF-IDF values for a collection of documents using Python's scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee74f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "\n",
      "Feature names (vocabulary):\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus (collection of documents)\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a TF-IDF representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Print the feature names\n",
    "print(\"\\nFeature names (vocabulary):\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87811e58",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "\n",
    "- We import `TfidfVectorizer` from scikit-learn, which is a tool for converting a collection of raw documents into a matrix of TF-IDF features.\n",
    "- We define a sample corpus consisting of four documents.\n",
    "- We create an instance of `TfidfVectorizer`.\n",
    "- We fit the vectorizer to the corpus and transform the corpus into a TF-IDF representation using the `fit_transform` method.\n",
    "- We retrieve the feature names (vocabulary) using the `get_feature_names_out` method.\n",
    "- Finally, we print the TF-IDF representation and the feature names.\n",
    "\n",
    "This demonstrates how to compute TF-IDF values for a collection of documents using Python's scikit-learn library. TF-IDF is particularly useful for tasks such as text classification and information retrieval, where determining the relevance of words in documents is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b907bb",
   "metadata": {},
   "source": [
    "## TF-IDF Numerical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b06f06",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used to evaluate the importance of a word in a document relative to a collection of documents. It is commonly used in natural language processing (NLP) and information retrieval tasks. Let's break down the concept of TF-IDF using an example:\n",
    "\n",
    "Consider a collection of documents (corpus) consisting of three documents:\n",
    "\n",
    "1. Document 1: \"The cat sat on the mat.\"\n",
    "2. Document 2: \"The dog jumped over the fence.\"\n",
    "3. Document 3: \"The cat and the dog played together.\"\n",
    "\n",
    "Now, let's calculate the TF-IDF values for each term in these documents.\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "   - Term Frequency measures how frequently a term appears in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document.\n",
    "\n",
    "   Example:\n",
    "   - TF(\"cat\", Document 1) = 1/6 = 0.1667\n",
    "   - TF(\"dog\", Document 2) = 1/6 = 0.1667\n",
    "   - TF(\"the\", Document 3) = 2/7 ≈ 0.2857\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   - Inverse Document Frequency measures the rarity of a term across the entire document collection. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, plus one to avoid division by zero.\n",
    "\n",
    "   Example:\n",
    "   - IDF(\"cat\") = log(3/2) ≈ 0.4055\n",
    "   - IDF(\"dog\") = log(3/2) ≈ 0.4055\n",
    "   - IDF(\"the\") = log(3/3) = 0\n",
    "\n",
    "3. **TF-IDF Score**:\n",
    "   - TF-IDF is calculated by multiplying the TF and IDF values for each term.\n",
    "\n",
    "   Example:\n",
    "   - TF-IDF(\"cat\", Document 1) = TF(\"cat\", Document 1) * IDF(\"cat\") ≈ 0.1667 * 0.4055 ≈ 0.0676\n",
    "   - TF-IDF(\"dog\", Document 2) = TF(\"dog\", Document 2) * IDF(\"dog\") ≈ 0.1667 * 0.4055 ≈ 0.0676\n",
    "   - TF-IDF(\"the\", Document 3) = TF(\"the\", Document 3) * IDF(\"the\") ≈ 0.2857 * 0 ≈ 0\n",
    "\n",
    "The TF-IDF values provide a measure of the importance of each term in the context of each document and the entire document collection. Terms that appear frequently in a document but rarely in other documents will have higher TF-IDF values, indicating their significance in representing the content of that document.\n",
    "\n",
    "This example illustrates how TF-IDF is computed for terms in a document collection, taking into account both term frequency within a document and term rarity across the entire collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0e682",
   "metadata": {},
   "source": [
    "The TF-IDF (Term Frequency-Inverse Document Frequency) formula calculates the importance of a term in a document relative to a collection of documents. It consists of two main components: Term Frequency (TF) and Inverse Document Frequency (IDF). The TF-IDF score for a term in a document is the product of its TF and IDF values. \n",
    "\n",
    "1. **Term Frequency (TF):**\n",
    "   Term Frequency measures how frequently a term appears in a document. It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document. It is usually normalized to prevent bias towards longer documents. The formula for TF is:\n",
    "\n",
    "   $\\ \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \\ $\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):**\n",
    "   Inverse Document Frequency measures the rarity of a term across the entire document collection. It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the term, plus one to avoid division by zero. The formula for IDF is:\n",
    "\n",
    "   $\\ \\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in corpus } N}{\\text{Number of documents containing term } t + 1}\\right) \\  $\n",
    "\n",
    "3. **TF-IDF Score:**\n",
    "   The TF-IDF score for a term \\( t \\) in a document \\( d \\) is the product of its TF and IDF values. The formula for TF-IDF is:\n",
    "\n",
    " $\\ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) \\ $\n",
    "\n",
    "   where \\( D \\) is the entire document collection (corpus).\n",
    "\n",
    "By combining the TF and IDF components, the TF-IDF score identifies terms that are both frequent within a document and rare across the entire document collection, which are likely to be important and discriminative for the document's content.\n",
    "\n",
    "It's important to note that there are variations of the TF-IDF formula, such as using sublinear scaling for TF, smoothing techniques for IDF, or adding normalization factors, depending on specific implementations and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4c947",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f136fff",
   "metadata": {},
   "source": [
    "N-grams are contiguous sequences of n items (words, characters, etc.) in a text. They are commonly used in natural language processing (NLP) for tasks such as language modeling, text generation, and feature extraction. Here's an explanation of N-grams along with a code example in Python:\n",
    "\n",
    "### Explanation of N-grams:\n",
    "- **Unigrams (1-grams)**: Single words in a text.\n",
    "- **Bigrams (2-grams)**: Contiguous sequences of two words.\n",
    "- **Trigrams (3-grams)**: Contiguous sequences of three words.\n",
    "- **N-grams**: Contiguous sequences of n words or characters.\n",
    "\n",
    "N-grams capture the local word order information in a text. For example, the bigram \"natural language\" in the sentence \"Natural language processing is interesting\" provides information about the relationship between the words \"natural\" and \"language.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75ce3bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams (1-grams): ['Natural', 'language', 'processing', 'is', 'interesting']\n",
      "Bigrams (2-grams): [('Natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'interesting')]\n",
      "Trigrams (3-grams): [('Natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'interesting')]\n"
     ]
    }
   ],
   "source": [
    "### Code Example in Python using NLTK:\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing is interesting\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate bigrams from the tokenized words\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "\n",
    "# Generate trigrams from the tokenized words\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "# Generate unigrams (words)\n",
    "unigrams = tokens\n",
    "\n",
    "# Print the generated n-grams\n",
    "print(\"Unigrams (1-grams):\", unigrams)\n",
    "print(\"Bigrams (2-grams):\", bigrams)\n",
    "print(\"Trigrams (3-grams):\", trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5213c0",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We import NLTK and necessary modules (`nltk.util.ngrams` for generating n-grams and `nltk.tokenize.word_tokenize` for tokenization).\n",
    "- We define a sample text.\n",
    "- We tokenize the text into words using `word_tokenize`.\n",
    "- We generate bigrams, trigrams, and unigrams (words) using the `ngrams` function from NLTK.\n",
    "- Finally, we print the generated n-grams.\n",
    "\n",
    "This demonstrates how to generate N-grams (unigrams, bigrams, and trigrams) from a text using NLTK in Python. N-grams are useful for capturing local word order information and are widely used in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6323fe",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f3856",
   "metadata": {},
   "source": [
    "Part-of-Speech (POS) tagging is the process of assigning grammatical categories (such as noun, verb, adjective, etc.) to words in a sentence. It is an essential step in natural language processing (NLP) tasks as it helps in understanding the syntactic structure of a sentence. Here's an explanation of POS tagging along with a code example in Python using the NLTK library:\n",
    "\n",
    "### Explanation of Part-of-Speech (POS) Tagging:\n",
    "POS tagging involves analyzing the words in a sentence and labeling them with their corresponding parts of speech based on their context and grammatical relationships within the sentence. For example, in the sentence \"The cat sat on the mat,\" the POS tags would be:\n",
    "- \"The\": determiner (DT)\n",
    "- \"cat\": noun (NN)\n",
    "- \"sat\": verb (VBD)\n",
    "- \"on\": preposition (IN)\n",
    "- \"the\": determiner (DT)\n",
    "- \"mat\": noun (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de350401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tagged tokens: [('The', 'DT'), ('cat', 'NN'), ('sat', 'VBD'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "### Code Example in Python using NLTK:\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech (POS) tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the POS tagged tokens\n",
    "print(\"POS tagged tokens:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54b062",
   "metadata": {},
   "source": [
    "\n",
    "In this code example:\n",
    "- We import NLTK and necessary modules (`nltk.tokenize.word_tokenize` for tokenization and `nltk.pos_tag` for POS tagging).\n",
    "- We define a sample sentence.\n",
    "- We tokenize the sentence into words using `word_tokenize`.\n",
    "- We perform POS tagging on the tokenized words using `pos_tag`.\n",
    "- Finally, we print the POS tagged tokens.\n",
    "\n",
    "This demonstrates how to perform Part-of-Speech (POS) tagging on a sentence using NLTK in Python. POS tagging is important for many NLP tasks, such as text parsing, information extraction, and syntactic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02549578",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb769ee",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is the process of identifying and categorizing named entities (such as person names, locations, organizations, dates, etc.) in a text. It helps in extracting structured information from unstructured text data. Here's an explanation of Named Entity Recognition along with a code example in Python using the NLTK library:\n",
    "\n",
    "### Explanation of Named Entity Recognition (NER):\n",
    "Named entities are specific pieces of information that are referred to by proper names in a text. NER involves identifying and categorizing these named entities into predefined categories such as person names, organization names, locations, etc. For example, in the sentence \"Apple is headquartered in Cupertino,\" the named entities would be:\n",
    "- \"Apple\": organization\n",
    "- \"Cupertino\": location\n",
    "\n",
    "Named Entities:\n",
    "Apple: ORGANIZATION\n",
    "Cupertino: GPE\n",
    "California: GPE\n",
    "Tim Cook: PERSON\n",
    "Apple Inc.: ORGANIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b173b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\omsai\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "Apple: GPE\n",
      "Cupertino: GPE\n",
      "California: GPE\n",
      "Tim Cook: PERSON\n",
      "CEO: ORGANIZATION\n",
      "Apple Inc: PERSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "### Code Example in Python using NLTK:\n",
    "\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "# Sample text\n",
    "text = \"Apple is headquartered in Cupertino, California. Tim Cook is the CEO of Apple Inc.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Perform named entity recognition (NER)\n",
    "ner_tags = nltk.pos_tag(tokens)\n",
    "ner_chunks = nltk.ne_chunk(ner_tags)\n",
    "\n",
    "# Extract named entities from the NER chunks\n",
    "named_entities = []\n",
    "for chunk in ner_chunks:\n",
    "    if isinstance(chunk, nltk.tree.Tree):\n",
    "        entity = \" \".join([token[0] for token in chunk])\n",
    "        entity_type = chunk.label()\n",
    "        named_entities.append((entity, entity_type))\n",
    "\n",
    "# Print the named entities and their types\n",
    "print(\"Named Entities:\")\n",
    "for entity, entity_type in named_entities:\n",
    "    print(f\"{entity}: {entity_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c0870",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We import NLTK.\n",
    "- We define a sample text containing named entities.\n",
    "- We tokenize the text into words using `nltk.word_tokenize`.\n",
    "- We perform part-of-speech (POS) tagging using `nltk.pos_tag`.\n",
    "- We perform named entity recognition (NER) using `nltk.ne_chunk`, which labels named entities with their entity types.\n",
    "- We extract named entities from the NER chunks and store them along with their entity types.\n",
    "- Finally, we print the named entities and their types.\n",
    "\n",
    "This demonstrates how to perform Named Entity Recognition (NER) on a text using NLTK in Python. NER is useful for various applications such as information extraction, question answering, and document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45a4b6",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724df560",
   "metadata": {},
   "source": [
    "Dependency Parsing is the process of analyzing the grammatical structure of a sentence to determine the relationships between words. It represents the syntactic structure of a sentence as a dependency tree, where each word is a node, and the dependencies between words are represented as directed edges.\n",
    "\n",
    "In a dependency tree:\n",
    "- Each word is a node representing a token in the sentence.\n",
    "- Each directed edge represents a grammatical relationship (dependency) between two words.\n",
    "- The head of the dependency (the word governing the relationship) is the parent node, and the dependent (the word being governed) is the child node.\n",
    "\n",
    "Here's an explanation of Dependency Parsing along with a code example in Python using the spaCy library:\n",
    "\n",
    "### Explanation of Dependency Parsing:\n",
    "Dependency Parsing helps in understanding the syntactic structure of a sentence by identifying the grammatical relationships between words. These relationships include subject-verb, verb-object, modifier-noun, etc. Dependency parsing can provide insights into the semantic relationships and the syntactic hierarchy within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8645944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tDependency\tHead Token\n",
      "The\tdet\t\tfox\n",
      "quick\tamod\t\tfox\n",
      "brown\tamod\t\tfox\n",
      "fox\tnsubj\t\tjumps\n",
      "jumps\tROOT\t\tjumps\n",
      "over\tprep\t\tjumps\n",
      "the\tdet\t\tdog\n",
      "lazy\tamod\t\tdog\n",
      "dog\tpobj\t\tover\n",
      ".\tpunct\t\tjumps\n"
     ]
    }
   ],
   "source": [
    "### Code Example in Python using spaCy:\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Perform dependency parsing\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the tokens and their dependencies\n",
    "print(\"Token\\tDependency\\tHead Token\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}\\t{token.dep_}\\t\\t{token.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f9ca9",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We import spaCy.\n",
    "- We load the English language model using `spacy.load`.\n",
    "- We define a sample sentence.\n",
    "- We perform dependency parsing using spaCy's `nlp` pipeline.\n",
    "- We iterate through the tokens in the parsed document and print each token along with its dependency relation (`token.dep_`) and its head token (`token.head.text`).\n",
    "\n",
    "This demonstrates how to perform Dependency Parsing on a sentence using the spaCy library in Python. Dependency Parsing is crucial for various NLP tasks such as information extraction, text summarization, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95e45b",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a41217",
   "metadata": {},
   "source": [
    "Word Embeddings are dense vector representations of words in a continuous vector space, where semantically similar words are mapped to nearby points. They capture the semantic meaning of words and their relationships with other words in the vocabulary. Word embeddings are commonly used in natural language processing (NLP) tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "One popular technique for generating word embeddings is Word2Vec, which learns word embeddings by training neural networks on large text corpora. Another popular technique is GloVe (Global Vectors for Word Representation), which learns word embeddings by factorizing a word-context matrix.\n",
    "\n",
    "Here's an explanation of Word Embeddings along with a code example in Python using the Gensim library for training Word2Vec embeddings:\n",
    "\n",
    "### Explanation of Word Embeddings:\n",
    "Word Embeddings represent words as **dense vectors** in a **continuous vector space**, where the similarity between words is captured by the **proximity of their vector representations**. These embeddings are learned from large text corpora using techniques such as Word2Vec, GloVe, or fastText. Word embeddings encode **semantic relationships between words**, enabling algorithms to capture meaning from text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d422da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'tasks': [-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03\n",
      "  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03\n",
      "  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03\n",
      " -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03\n",
      "  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03\n",
      " -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03\n",
      "  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03\n",
      " -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03\n",
      "  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03\n",
      " -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04\n",
      " -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03\n",
      " -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03\n",
      " -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03\n",
      "  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03\n",
      " -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03\n",
      " -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03\n",
      " -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03\n",
      " -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03\n",
      "  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05\n",
      "  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04\n",
      " -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03\n",
      " -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03\n",
      " -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03\n",
      "  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04\n",
      "  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03]\n",
      "Similar Words to 'tasks': [('natural', 0.15016482770442963), ('in', 0.12813478708267212), ('embeddings', 0.0931011363863945), ('learning', 0.09216003865003586), ('are', 0.04652618616819382), ('processing', 0.0007306992192752659), ('word', -0.0036444426514208317), ('algorithms', -0.0037013492546975613), ('love', -0.00925341248512268), ('NLP', -0.030302340164780617)]\n"
     ]
    }
   ],
   "source": [
    "### Code Example in Python using Gensim (Word2Vec):\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    word_tokenize(\"I love natural language processing\"),\n",
    "    word_tokenize(\"Word embeddings are useful in NLP tasks\"),\n",
    "    word_tokenize(\"Machine learning algorithms learn word embeddings\"),\n",
    "]\n",
    "\n",
    "# Train Word2Vec model on the corpus\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv[\"tasks\"]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"tasks\")\n",
    "\n",
    "# Print the word vector and similar words\n",
    "print(\"Word Vector for 'tasks':\", word_vector)\n",
    "print(\"Similar Words to 'tasks':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064a47d",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We import the Word2Vec model from Gensim and the word_tokenize function from NLTK for tokenization.\n",
    "- We define a sample corpus consisting of tokenized sentences.\n",
    "- We train a Word2Vec model on the corpus using Gensim's Word2Vec class.\n",
    "- We retrieve the word vector for a specific word (\"word\") using the model's word vectors (model.wv).\n",
    "- We find similar words to a given word (\"word\") using the most_similar method of the model's word vectors.\n",
    "- Finally, we print the word vector for the given word and the similar words to it.\n",
    "\n",
    "This demonstrates how to train and use Word Embeddings using Word2Vec with the Gensim library in Python. Word embeddings enable algorithms to understand the semantic relationships between words in a text corpus, facilitating various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a5e4f",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14e0ee",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique for learning word embeddings from large text corpora. It represents words as dense vectors in a continuous vector space, where similar words are mapped to nearby points. Word2Vec captures semantic relationships between words by learning to predict a target word based on its context words (skip-gram model) or to predict context words based on a target word (continuous bag of words (CBOW) model).\n",
    "\n",
    "Here's an explanation of Word2Vec along with a code example in Python using the Gensim library:\n",
    "\n",
    "### Explanation of Word2Vec:\n",
    "Word2Vec learns word embeddings by training neural networks on large text corpora. It operates on the principle of the distributional hypothesis, which suggests that words that appear in similar contexts tend to have similar meanings. Word2Vec models learn to capture these contextual similarities by mapping words to high-dimensional vectors in a continuous vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d9b0503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'space': [-8.2471324e-03  9.3018534e-03 -1.9942678e-04 -1.9626648e-03\n",
      "  4.6076379e-03 -4.1001830e-03  2.7420276e-03  6.9454350e-03\n",
      "  6.0622939e-03 -7.5120688e-03  9.3771974e-03  4.6687461e-03\n",
      "  3.9647501e-03 -6.2395968e-03  8.4699327e-03 -2.1527063e-03\n",
      "  8.8227512e-03 -5.3617889e-03 -8.1372000e-03  6.8197437e-03\n",
      "  1.6723435e-03 -2.1968586e-03  9.5197167e-03  9.4881048e-03\n",
      " -9.7697265e-03  2.5014866e-03  6.1513479e-03  3.8709873e-03\n",
      "  2.0187970e-03  4.2898551e-04  6.8217382e-04 -3.8221811e-03\n",
      " -7.1351537e-03 -2.0916499e-03  3.9208545e-03  8.8176951e-03\n",
      "  9.2581352e-03 -5.9689130e-03 -9.4042262e-03  9.7582126e-03\n",
      "  3.4249516e-03  5.1680170e-03  6.2800436e-03 -2.8035773e-03\n",
      "  7.3228464e-03  2.8319580e-03  2.8675152e-03 -2.3763445e-03\n",
      " -3.1271677e-03 -2.3675051e-03  4.2794747e-03  7.3227791e-05\n",
      " -9.5831258e-03 -9.6651195e-03 -6.1527668e-03 -1.2934914e-04\n",
      "  1.9992781e-03  9.4242264e-03  5.5782967e-03 -4.2867940e-03\n",
      "  2.8398441e-04  4.9654548e-03  7.7038780e-03 -1.1458711e-03\n",
      "  4.3209363e-03 -5.8124391e-03 -8.0064888e-04  8.1030121e-03\n",
      " -2.3680783e-03 -9.6577499e-03  5.7789851e-03 -3.9292388e-03\n",
      " -1.2195364e-03  9.9814488e-03 -2.2551054e-03 -4.7542509e-03\n",
      " -5.3239283e-03  6.9829165e-03 -5.7109683e-03  2.1066710e-03\n",
      " -5.2538584e-03  6.1194086e-03  4.3530301e-03  2.6077894e-03\n",
      " -1.4952403e-03 -2.7443713e-03  8.9946240e-03  5.2127130e-03\n",
      " -2.1626682e-03 -9.4688162e-03 -7.4199587e-03 -1.0606997e-03\n",
      " -7.9415803e-04 -2.5645979e-03  9.6859271e-03 -4.5395221e-04\n",
      "  5.8676749e-03 -7.4506840e-03 -2.5023650e-03 -5.5523878e-03]\n",
      "Similar Words to 'space': [('their', 0.17818619310855865), ('missions', 0.1637497842311859), ('landmark', 0.14945359528064728), ('amateur', 0.1352025270462036), ('to', 0.13163886964321136), ('eight', 0.09724962711334229), ('objects', 0.07777492702007294), ('for', 0.07527992129325867), ('in', 0.06806699931621552), ('landing', 0.06761940568685532)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    [\"Space\", \"exploration\", \"has\", \"opened\", \"up\", \"new\", \"possibilities\", \"for\", \"humanity\"],\n",
    "    [\"Astronomers\", \"study\", \"the\", \"cosmos\", \"to\", \"understand\", \"the\", \"universe\"],\n",
    "    [\"The\", \"astronauts\", \"observed\", \"distant\", \"galaxies\", \"during\", \"their\", \"mission\"],\n",
    "    [\"Stargazing\", \"is\", \"a\", \"popular\", \"activity\", \"for\", \"amateur\", \"astronomers\"],\n",
    "    [\"The\", \"solar\", \"system\", \"consists\", \"of\", \"the\", \"sun\", \"and\", \"eight\", \"planets\"],\n",
    "    [\"Black\", \"holes\", \"are\", \"mysterious\", \"and\", \"fascinating\", \"objects\", \"in\", \"space\"],\n",
    "    [\"NASA\", \"aims\", \"to\", \"explore\", \"Mars\", \"in\", \"upcoming\", \"missions\"],\n",
    "    [\"The\", \"moon\", \"landing\", \"was\", \"a\", \"landmark\", \"achievement\", \"in\", \"space\", \"exploration\"],\n",
    "]\n",
    "\n",
    "# Train Word2Vec model on the corpus\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv[\"space\"]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"space\")\n",
    "\n",
    "# Print the word vector and similar words\n",
    "print(\"Word Vector for 'space':\", word_vector)\n",
    "print(\"Similar Words to 'space':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18c976",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We import the Word2Vec model from Gensim and the word_tokenize function from NLTK for tokenization.\n",
    "- We define a sample corpus consisting of tokenized sentences.\n",
    "- We train a Word2Vec model on the corpus using Gensim's Word2Vec class.\n",
    "- We retrieve the word vector for a specific word (\"word\") using the model's word vectors (model.wv).\n",
    "- We find similar words to a given word (\"word\") using the most_similar method of the model's word vectors.\n",
    "- Finally, we print the word vector for the given word and the similar words to it.\n",
    "\n",
    "This demonstrates how to train and use Word2Vec embeddings using the Gensim library in Python. Word2Vec embeddings capture semantic relationships between words and are widely used in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e1af2",
   "metadata": {},
   "source": [
    "# GloVe (Global Vectors for Word Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96128472",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. It was proposed by Stanford researchers Pennington, Socher, and Manning in 2014. GloVe learns vector representations by leveraging the global statistics of word co-occurrence frequencies in a corpus.\n",
    "\n",
    "Here's an explanation of GloVe along with a code example in Python using the Gensim library:\n",
    "\n",
    "### Explanation of GloVe:\n",
    "GloVe learns word vectors by considering the global co-occurrence statistics of words in a large text corpus. It constructs a co-occurrence matrix, where each element \\( X_{ij} \\) represents how often word \\( i \\) appears in the context of word \\( j \\) in the corpus. GloVe then factorizes this co-occurrence matrix to obtain word vectors that capture semantic relationships between words.\n",
    "\n",
    "### Code Example in Python using Gensim:\n",
    "Gensim does not directly support training GloVe models. However, you can use the `gensim.scripts.glove2word2vec` module to convert pre-trained GloVe vectors to the Word2Vec format, which can then be loaded and used with Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c21d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omsai\\AppData\\Local\\Temp\\ipykernel_11992\\3378089380.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'king': [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n",
      "Similar Words to 'king': [('prince', 0.7682329416275024), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775828361511), ('monarch', 0.6977890729904175), ('throne', 0.691999077796936), ('kingdom', 0.6811409592628479), ('father', 0.6802029013633728), ('emperor', 0.6712858080863953), ('ii', 0.6676074266433716)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Convert GloVe format to Word2Vec format\n",
    "glove_input_file = r'C:\\Users\\omsai\\velocity\\Daily Notes_NLP (Ankita Unhale Maam)\\glove.6B.100d.txt'  # Path to GloVe file\n",
    "word2vec_output_file = 'glove.6B.100d.word2vec.txt'  # Output path for Word2Vec file\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load GloVe word vectors as Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model[\"king\"]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.most_similar(\"king\")\n",
    "\n",
    "# Print the word vector and similar words\n",
    "print(\"Word Vector for 'king':\", word_vector)\n",
    "print(\"Similar Words to 'king':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57684d",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We convert pre-trained GloVe word vectors from the GloVe format to the Word2Vec format using the `glove2word2vec` function from the `gensim.scripts.glove2word2vec` module.\n",
    "- We load the converted GloVe word vectors as a Word2Vec model using Gensim's `KeyedVectors.load_word2vec_format` method.\n",
    "- We retrieve the word vector for the word \"king\" using the loaded Word2Vec model.\n",
    "- We find similar words to the word \"king\" using the `most_similar` method of the Word2Vec model.\n",
    "- Finally, we print the word vector for \"king\" and the similar words to it.\n",
    "\n",
    "This demonstrates how to load and use pre-trained GloVe word vectors with Gensim in Python. GloVe word vectors capture semantic relationships between words based on global co-occurrence statistics in a large text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbf27b",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa9c09",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence (Seq2Seq) is a model architecture used in natural language processing (NLP) for tasks such as machine translation, text summarization, and conversational agents. It consists of two main components: an encoder and a decoder. The encoder processes the input sequence (source sequence) and converts it into a fixed-size context vector, capturing the input sequence's semantic information. The decoder then takes this context vector and generates the output sequence (target sequence) one step at a time, predicting the next word based on the previous words and the context vector.\n",
    "\n",
    "Here's an explanation of Sequence-to-Sequence (Seq2Seq) along with a code example in Python using the TensorFlow library:\n",
    "\n",
    "### Explanation of Sequence-to-Sequence (Seq2Seq):\n",
    "Seq2Seq models are designed to handle variable-length input and output sequences. They are widely used in tasks where the length of the input and output sequences may differ, such as machine translation or text summarization.\n",
    "\n",
    "In Seq2Seq models:\n",
    "- The encoder processes the input sequence and produces a fixed-size context vector.\n",
    "- The decoder takes this context vector and generates the output sequence step by step.\n",
    "\n",
    "The key idea behind Seq2Seq models is to use a fixed-size context vector to capture the input sequence's semantic information, which is then used by the decoder to generate the output sequence.\n",
    "\n",
    "### Code Example in Python using TensorFlow:\n",
    "Here's a simple code example of a Seq2Seq model for machine translation using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf4db293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 11s 21ms/step - loss: 7.9423 - val_loss: 4.7208\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 3.2254 - val_loss: 2.9946\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 4s 16ms/step - loss: 2.0681 - val_loss: 1.6414\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 1.5434 - val_loss: 1.6062\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 1.2835 - val_loss: 1.4238\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 1.1392 - val_loss: 0.9233\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.9537 - val_loss: 0.8005\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.8620 - val_loss: 0.8945\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.8130 - val_loss: 0.6561\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.7196 - val_loss: 0.5998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2dd5d073010>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Function to generate training data\n",
    "def generate_data(num_samples, max_sequence_length):\n",
    "    X = np.random.randint(0, 10, size=(num_samples, max_sequence_length))\n",
    "    Y = np.array([list(reversed(x)) for x in X])\n",
    "    return X, Y\n",
    "\n",
    "# Define Seq2Seq model\n",
    "def seq2seq_model(input_shape, output_sequence_length):\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    encoder = LSTM(64, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(output_sequence_length, 1))\n",
    "    decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(1, activation='linear')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Generate training data\n",
    "num_samples = 10000\n",
    "max_sequence_length = 10\n",
    "X, Y = generate_data(num_samples, max_sequence_length)\n",
    "\n",
    "# Reshape data for the Seq2Seq model\n",
    "X = X.reshape((num_samples, max_sequence_length, 1))\n",
    "Y = Y.reshape((num_samples, max_sequence_length, 1))\n",
    "\n",
    "# Create and compile the Seq2Seq model\n",
    "model = seq2seq_model(input_shape=X.shape, output_sequence_length=max_sequence_length)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Adjust the decoder input and output sequences\n",
    "decoder_input_seq = np.concatenate([np.zeros((num_samples, 1, 1)), Y[:, :-1, :]], axis=1)\n",
    "decoder_output_seq = Y\n",
    "\n",
    "# Train the model\n",
    "model.fit([X, decoder_input_seq], decoder_output_seq, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35eb2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Original Sequence: [4 9 8 0 9 4 4 8 3 6]\n",
      "Predicted Sequence: [ 6 13 17 19 19 18 18 18 18 18]\n",
      "-----\n",
      "Original Sequence: [3 1 6 2 1 0 3 3 3 1]\n",
      "Predicted Sequence: [ 1  5 10 14 15 16 16 16 16 15]\n",
      "-----\n",
      "Original Sequence: [0 8 0 6 6 2 9 0 2 7]\n",
      "Predicted Sequence: [ 7 13 17 18 18 18 17 17 17 17]\n",
      "-----\n",
      "Original Sequence: [9 4 1 1 2 1 7 1 2 4]\n",
      "Predicted Sequence: [ 3  8 13 16 17 16 16 16 16 16]\n",
      "-----\n",
      "Original Sequence: [8 7 6 8 7 1 2 7 9 3]\n",
      "Predicted Sequence: [ 4 12 17 19 19 19 18 18 18 18]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Generate some random test data\n",
    "num_test_samples = 5\n",
    "X_test, Y_test = generate_data(num_test_samples, max_sequence_length)\n",
    "\n",
    "# Reshape test data\n",
    "X_test = X_test.reshape((num_test_samples, max_sequence_length, 1))\n",
    "Y_test = Y_test.reshape((num_test_samples, max_sequence_length, 1))\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_sequences = model.predict([X_test, np.zeros((num_test_samples, max_sequence_length, 1))])\n",
    "\n",
    "# Print the original and predicted sequences\n",
    "for i in range(num_test_samples):\n",
    "    print(f\"Original Sequence: {X_test[i,:,0]}\")\n",
    "    print(f\"Predicted Sequence: {predicted_sequences[i,:,0].round().astype(int)}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be69f0",
   "metadata": {},
   "source": [
    "In this code example:\n",
    "- We define the encoder and decoder components of the Seq2Seq model using TensorFlow's Keras API.\n",
    "- The encoder takes input sequences and produces context vectors using an embedding layer and an LSTM layer.\n",
    "- The decoder takes input sequences and the context vectors and generates output sequences using an embedding layer, an LSTM layer, and a dense layer.\n",
    "- We define the Seq2Seq model by connecting the encoder and decoder components.\n",
    "- We compile the model with an optimizer and loss function.\n",
    "- We train the model using dummy data for demonstration purposes.\n",
    "\n",
    "This code demonstrates a basic Seq2Seq model for machine translation using TensorFlow's Keras API. However, real-world applications may require more complex architectures and additional techniques, such as attention mechanisms, beam search, or teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34c204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93810652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91852f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202b9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ffd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d5e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a91dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
